{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2badf1-8fb8-4ae4-8d00-ca1b0ef5d019",
   "metadata": {},
   "source": [
    "K-NEARST NEIGHBORS ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ff7de-8b41-4195-8eb0-faae235a7b2c",
   "metadata": {},
   "source": [
    "PART 1 - DISTANCE CALCULATION\n",
    "- The core of KNN inolves measure distances between data points. In this step W'ill create a function to compute the Euclidian Distance between two points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8b8cd-fab7-41ec-af18-c0a5615156f6",
   "metadata": {},
   "source": [
    "THE EUCLIDIAN DISTANCE BETWEEN TWO POINTS"
   ]
  },
  {
   "attachments": {
    "7d4069c9-4101-4c92-88cb-fc6d6a03eac2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d4dc4c80-ae5e-4b74-89e9-89a5722aeb55",
   "metadata": {},
   "source": [
    "![Euclidian.png](attachment:7d4069c9-4101-4c92-88cb-fc6d6a03eac2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f1fda-c5b4-413e-8f10-4635b067a5b1",
   "metadata": {},
   "source": [
    " - Calculate the Euclidian Distance between two points: \n",
    "\n",
    " - Parameters:\n",
    "\n",
    "   \n",
    "      point 1: list or arrays-like (coordinates of the first point)\n",
    "\n",
    "      point 2: list or arrays-like (coordinates of the second point)\n",
    "    \n",
    " - Returns: \n",
    "    distance: float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "018e5977-216f-483d-b5bf-a53b0f4af97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(point1, point2):\n",
    "    #Ensure both points have same dimension\n",
    "    assert len(point1) == len(point2), 'Point must have the same dimension' #assert statment: 'Hey Python, the following affirmation must be truth. Otherwise, return an assert error with the following string text ' ... '. The same can be done with the \"if ; raise\" statment\n",
    "   \n",
    "    #Compute the Euclidian Distance:\n",
    "    # 1\n",
    "    e_distance = 0\n",
    "    for i in range(len(point1)):\n",
    "        dif = (point2[i] - point1[i])**2 # (q - p) --> 'q' query point; the new entry point / 'p' target point; the known point; the know value from the data base. [but, the order dosent metter because of the square²]\n",
    "        e_distance += dif\n",
    "    e_distance = e_distance**0.5\n",
    "    \n",
    "    \"\"\"\n",
    "    # 2\n",
    "    e_distance = (sum((q[i] - p[i])**2 for i in range(len(p))))**0.5\n",
    "\n",
    "    # 3\n",
    "    e_distance = sum((p1 - p2)**2 for p1, p2 in zip(point1, point2))**0.5\n",
    "    \"\"\"\n",
    "    return e_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a96ec8-6b57-45c2-9054-416a369c34b5",
   "metadata": {},
   "source": [
    "PART 2 - FIND THE NEAREST NEIGHBORS\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the 'K' nearest neighbors of a query point within a dataset.\n",
    "    Parameters:\n",
    "        x_train: list/vector or array-like \n",
    "            # Training dataset containg features\n",
    "        query_point: list/vector or array-like\n",
    "            # Coordinates of the query point \n",
    "        K: int\n",
    "            # Number of neighbors to find\n",
    "    Returns:\n",
    "    neighbors: list\n",
    "        # List object with the indices of the 'K' nearest neighbors\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20d68c84-2074-4686-afdd-86b5164e901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(x_train, query_point, k): # Parameters: 'x_train' - training dataset / 'query_point' - the input you want to classify / 'k' - number of neighbors to find\n",
    "    distance = []\n",
    "\n",
    "    # calculate the distance between the query point to each point in the training set - one by one\n",
    "    for i, data_point in enumerate(x_train):\n",
    "        distance = euclidian_distance(query_point, data_point) \n",
    "        distance.append((i, distance)) # add to the 'distance' empty list a tuple formed by the index of each value analyzed in the training set and the euclidian distance between the points\n",
    "\n",
    "    # Sort distances in ascending order\n",
    "    distance.sort(key=lambda x: x[1]) # will organize the values of 'distance' by a crescent order acessing each tuple of the list 'x:', and then each second position value x[1] (that is the distance value)\n",
    "                                      # will organize by the value of the distance (lower to high), but the original index still registred at the tuple e.g (3, 1.2), (1, 1.5), (2, 1.9), ...\n",
    "    # get the indices of the 'k' nearest neighbors\n",
    "    neighbors = []\n",
    "    top_k = distance[ : k] # list slicing - top_k stores tuples (index, distance value) slicing from the beginning of the list till the k-1 (wich is the number of chosen neighbors to find at the beginning)\n",
    "\n",
    "    for item in top_k: # scroll through the top_k list, while 'item' assumes each stored tuple\n",
    "        index = item[0] # then, get the first positioning value at the tuple (that is the index value) \n",
    "        neighbors.append(index) # and add this index (the positioning of the k value in the original distance list) to the 'neighbors' empty list\n",
    "    return neighbors # Therefore, neighbors is a list that stores the indexes of the lowest values from 'distances' (these are the 'Nearest Neighbors' between the query point and the x_train data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9377a1-59e6-4583-b393-fbb03b23e877",
   "metadata": {},
   "source": [
    "PART 3 - PREDICTING THE CLASS\n",
    "\n",
    "    \"\"\"\n",
    "        Predicting the class of a query point based on the majority class among its nearest neighbors\n",
    "        Parameters:\n",
    "        x_train: list/vector or array-like\n",
    "            Training dataset containing features\n",
    "        y_train: list/ vector or array-like\n",
    "            Training dataset containing labels. \n",
    "        query_point: list/vector or array-like\n",
    "            Coordinates of the query point\n",
    "        k: int\n",
    "        number of neighbors to consider\n",
    "        returns: \n",
    "        predicted_class: int or str\n",
    "            Predicted class label for the query point\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada280a9-0bcf-41bc-b372-8197a30e6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_train, y_train, query_point, k): \n",
    "    # It will try to predict a label (1 or 0) of a query_point \n",
    "    # Performs the actual classification by comparing the query to each x_train data and its corresponding labels in y_train \n",
    "    \n",
    "    neighbors = find_neighbors(x_train, query_point, k) \n",
    "    # Calls the function that returns the indexes of 'k' nearest neighbors - the “closest” (the most similar to our query).\n",
    "    # These indexes will be used to find the corresponding labels (1 or 0)\n",
    "    \n",
    "    neighbors_labels = [] \n",
    "    for i in neighbors:\n",
    "        neighbors_labels.append(y_train[i])\n",
    "    # Here, acessing y_train, I get the labels (1 or 0) of the corresponding indexes in 'neighbors' list\n",
    "\n",
    "    count_ones = neighbors_labels.count(1) # how many ones '1' are present in 'neighbors_labels'\n",
    "    \n",
    "    if count_ones > len(neighbors_labels) // 2:\n",
    "    # if the number of ones it's more than half of the 'neighbors_labels', then '1's are majority neighbors to the query point.\n",
    "        predicted_class = 1 # therefore, the query point is classified as '1' as well - true.\n",
    "    else: \n",
    "        predicted_class = 0 # else, if it's minority, it's classified as '0'\n",
    "\n",
    "    return predicted_class     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfb627-8e40-4e2f-89aa-1b0b81d40a87",
   "metadata": {},
   "source": [
    "    CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e405c7c-3ff8-4932-acb5-753bf368c5ca",
   "metadata": {},
   "source": [
    " - LOAD/LOOK/CLEAN THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941b6e5f-9d55-4a87-aea6-ab2a1acc4575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data_load = pd.read_csv('news_labeled_dataset_II.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfe87a39-040d-4bbc-bcaf-3e87acd53c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning: (300, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f'Before Cleaning: {data_load.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89e0b38e-88c0-4f22-858c-75d1f6c4be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraud = data_load.iloc[:150]\n",
    "data_nfraud = data_load.iloc[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499f05f0-30d4-4434-a385-f2dbc22fde84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                             150\n",
       "label                                                            1\n",
       "titulo           Transporte escolar em Santa Catarina é alvo de...\n",
       "data                                                    01/08/2023\n",
       "texto_noticia    Transporte escolar em Santa Catarina é alvo de...\n",
       "link_noticia     https://www.tvbv.com.br/transporte-escolar-em-...\n",
       "Name: 149, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_load.iloc[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a890a74-6729-42ae-be83-b93c28a8612a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2417572-b6e1-41c3-94a4-083c2e62f243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re # regular expressions to handle with text patterns\n",
    "import string # to help with cleaning punctuiations\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # convert text into numeric values\n",
    "from sklearn.model_selection import train_test_split # training and test samples\n",
    "from sklearn.neighbors import KNeighborsClassifier # the KNN from Scikit-Learn \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score # to evaluate how accurate the predictions are \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "\n",
    "def read_data(file_name):\n",
    "    # Load the dataset with comma delimimiter\n",
    "    data = pd.read_csv(file_name, delimiter=',')\n",
    "    return data\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower() #converts everything into string, then into lowercase\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text) # \"replace(pattern, replacement, original_text)\" - removes all string ponctuations using regex\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # raw string r' '; whitespace \\s; one or more times +; replace them with one single space ' '; in text; also, remove spaces at beginning or and of the text .strip(). \n",
    "    return text # returns the clean text in: lowercase + without punctuations + without multiple withespaces\n",
    "\n",
    "def preprocess_text_data(data, custom_stopwords=None):\n",
    "    \n",
    "    data['full_text'] = data['titulo'].fillna('') + ' ' + data['texto_noticia'].fillna('') \n",
    "    # creates a new colum where it join the 'title' and 'text' columns from the dataset together (also cleans any possible NaN value by replacing it for a empty string ''\n",
    "    data['full_text'] = data['full_text'].apply(clean_text)\n",
    "    \n",
    "    stop_words_pt = stopwords.words('portuguese')\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words = stop_words_pt, max_features=1000)\n",
    "    X = vectorizer.fit_transform(data['full_text']) # [fit: learn from data / transform: convert data into numbers] result - matrix of numbers ready to train the model\n",
    "    Y = data['label'].fillna('')\n",
    "    return X, Y, vectorizer\n",
    "    \n",
    "def split_data(X, Y): \n",
    "    # 0.8 train / 0.2 test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    # Spliting with the labels (0 and 1) / using strtify \n",
    "        X,\n",
    "        Y,\n",
    "        train_size = 0.8,\n",
    "        shuffle = True,\n",
    "        random_state = 42,\n",
    "        stratify = Y\n",
    "    )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n",
    "def fit_model(x_train, y_train, k=5):\n",
    "    # “These 240 news articles are labeled: some are about fraud, others are not. \n",
    "    # learn to distinguish between them, using the words/features they contain.”\n",
    "\n",
    "    # Implement KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    # Predict on the train set (evaluate how well the training data was memorized)\n",
    "    preds_in_train = knn.predict(x_train)\n",
    "\n",
    "    #calculate the accuracy of preds on the train data\n",
    "    train_accuracy = accuracy_score(y_train, preds_in_train)\n",
    "\n",
    "    return train_accuracy, knn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d373dbae-f666-4657-97f7-b9e3e0d65a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy on train data: 0.9285714285714286\n",
      "Validation accuracy: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "#Run the Flow\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv('news_labeled_dataset_II.csv')\n",
    "\n",
    "# convert text into numbers\n",
    "X, Y, vectorizer = preprocess_text_data(data)\n",
    "\n",
    "# split the data (80/20)\n",
    "x_trainval, x_test, y_trainval, y_test = split_data(X, Y)\n",
    "\n",
    "# split the data again, now, the train data for validation(70/30) (divisão adicional entre treino e validação)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_trainval,\n",
    "    y_trainval,\n",
    "    train_size = 0.7,\n",
    "    random_state = 42,\n",
    "    stratify = y_trainval\n",
    ")\n",
    "\n",
    "# fit on train data\n",
    "train_accuracy, knn = fit_model(x_train, y_train)\n",
    "print(f'KNN accuracy on train data: {train_accuracy}')\n",
    "\n",
    "clas_preds = knn.predict(x_val) # classification predictions\n",
    "val_accuracy = accuracy_score(y_val, clas_preds) # compares the predicts with the original values to see the accuracy\n",
    "print(f'Validation accuracy: {val_accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "#JUST PRESS THE BUTTON AND SEE WHAT HAPPENS..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299cfece-6059-43bc-a79c-593e1973eb0a",
   "metadata": {},
   "source": [
    "DEU CERTOOOOOOOOOOOOOOOOOOOOOO!!!\n",
    "OBRIGADO MEU DEUS POR TUDO! \n",
    "VAMOS EM NOME DO SENHOR JESUS! AMÉM! \n",
    "SEMPRE!\n",
    "\n",
    "JUST\n",
    "KEEP\n",
    "GOING!\n",
    "\n",
    "29/04/2025 18:53\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0391ce78-f4ba-426d-aff1-29ede9a7d1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'metric': 'cosine', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "Validation accuracy 0.8989304812834223\n",
      "\n",
      "\n",
      "KNN accuracy on train data: 0.9285714285714286\n",
      "Validation accuracy: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "#Run the Flow\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv('news_labeled_dataset_II.csv')\n",
    "\n",
    "# convert text into numbers\n",
    "X, Y, vectorizer = preprocess_text_data(data)\n",
    "\n",
    "# split the data (80/20)\n",
    "x_trainval, x_test, y_trainval, y_test = split_data(X, Y)\n",
    "\n",
    "# split the data again, now, the train data for validation(70/30) (divisão adicional entre treino e validação)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_trainval,\n",
    "    y_trainval,\n",
    "    train_size = 0.7,\n",
    "    random_state = 42,\n",
    "    stratify = y_trainval\n",
    ")\n",
    "\n",
    "#  ***\n",
    "# find the best hypearparameters\n",
    "params = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11, 15],\n",
    "    'weights': ['uniform', 'distance'], \n",
    "    'metric': ['euclidean', 'manhattan', 'cosine', 'minkowski']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), params, cv=5)  #try to find the best hyperparameter\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Validation accuracy\", grid.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# fit on train data\n",
    "train_accuracy, knn = fit_model(x_train, y_train)\n",
    "print(f'KNN accuracy on train data: {train_accuracy}')\n",
    "\n",
    "clas_preds = knn.predict(x_val) # classification predictions\n",
    "val_accuracy = accuracy_score(y_val, clas_preds) # compares the predicts with the original values to see the accuracy\n",
    "print(f'Validation accuracy: {val_accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "#JUST PRESS THE BUTTON AND SEE WHAT HAPPENS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43be1be-299a-4a44-a2f1-4202e653a44d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'metric': 'euclidean', 'n_neighbors': 6, 'weights': 'uniform'}\n",
      "Validation accuracy 0.9344028520499108\n",
      "\n",
      "\n",
      "KNN accuracy on train data: 0.9285714285714286\n",
      "Validation accuracy: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "#Run the Flow\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv('news_labeled_dataset_II.csv')\n",
    "\n",
    "# convert text into numbers\n",
    "X, Y, vectorizer = preprocess_text_data(data)\n",
    "\n",
    "# split the data (80/20)\n",
    "x_trainval, x_test, y_trainval, y_test = split_data(X, Y)\n",
    "\n",
    "# split the data again, now, the train data for validation(70/30) (divisão adicional entre treino e validação)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_trainval,\n",
    "    y_trainval,\n",
    "    train_size = 0.7,\n",
    "    random_state = 42,\n",
    "    stratify = y_trainval\n",
    ")\n",
    "\n",
    "lista = list(range(1, 41))\n",
    "# find the best hypearparameters\n",
    "params = {\n",
    "    'n_neighbors': lista,\n",
    "    'weights': ['uniform', 'distance'], \n",
    "    'metric': ['euclidean', 'manhattan', 'cosine', 'minkowski']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), params, cv=5)  #try to find the best hyperparameter\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Validation accuracy\", grid.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# fit on train data\n",
    "train_accuracy, knn = fit_model(x_train, y_train)\n",
    "print(f'KNN accuracy on train data: {train_accuracy}')\n",
    "\n",
    "clas_preds = knn.predict(x_val) # classification predictions\n",
    "val_accuracy = accuracy_score(y_val, clas_preds) # compares the predicts with the original values to see the accuracy\n",
    "print(f'Validation accuracy: {val_accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "#JUST PRESS THE BUTTON AND S0.9344028520499108EE WHAT HAPPENS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992f548-adac-4393-91ac-7dc82903d77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
